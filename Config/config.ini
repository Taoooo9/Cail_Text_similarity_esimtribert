[Data]
train_file = Data/small/input_jieba_dev_small.txt
dev_file = Data/small/input_jieba_dev_small.txt
embedding_file = Data/skipgram.200d
stop_words = Data/stop_words.txt

[Save]
save_dir = snapshot
save_pkl_path = %(save_dir)s/pkl
save_analysis_pkl_path = %(save_dir)s/analysis_pkl
save_model_path = %(save_dir)s/model
save_vocab_path = %(save_dir)s/vocab
bert_model_pkl = %(save_dir)s/model/bert_model.pkl
esim_model_pkl = %(save_dir)s/model/esim_model.pkl
char_model_pkl = %(save_dir)s/model/char_model.pkl
train_data_analysis_pkl = %(save_dir)s/analysis_pkl/train_data.pkl
train_data_word_pkl = %(save_dir)s/pkl/train_word_data.pkl
dev_data_word_pkl = %(save_dir)s/pkl/dev_word_data.pkl
test_data_word_pkl = %(save_dir)s/pkl/test_word_data.pkl
train_data_sen_pkl = %(save_dir)s/pkl/train_sen_data.pkl
dev_data_sen_pkl = %(save_dir)s/pkl/dev_sen_data.pkl
test_data_sen_pkl = %(save_dir)s/pkl/test_sen_data.pkl
fact_word_vocab = %(save_dir)s/vocab/fact_word_vocab.pkl
fact_sen_vocab = %(save_dir)s/vocab/fact_sen_vocab.pkl
fact_char_vocab = %(save_dir)s/vocab/fact_char_vocab.pkl
embedding_pkl = %(save_dir)s/pkl/embedding.pkl

[Data_analysis]
analysis_data = False

[Train]
use_cuda = False
epoch = 1000
batch_size = 1
dev_batch_size = 1
use_lr_decay = False
clip_max_norm_use = False
test_interval = 20
early_stop = 10
less_triplet_loss = False
update_every = 4
n_fold = 5

[Data_loader]
read_sen = False
read_char = True
data_cut = True
stop_word = True

[Model]
embedding_char_dim = 100
embedding_word_dim = 200
embedding_sen_dim = 300
embedding_word_num = 1220
embedding_char_num = 3
embedding_sen_num = 4268
input_channels = 1
kernel_num = 100
kernel_size = 357
hidden_size = 300
dropout = 0.5
class_num = 2
which_model = LSTM
learning_algorithm = adam
bert_lr = 0.001
esim_lr = 0.001
weight_decay = 1.0e-8
lr_rate_decay = 0.05
margin = 1.0
p = 2
epsilon = 1.0e-8
pre_embedding = False
which = add
hidden_style = single
factor = 0.9
patience = 10
min_lr = 0.0004
max_sen_len = 510
correct_bias = False
k = 0

